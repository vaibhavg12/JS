{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import everything that is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace url with the wikipedia site you wish to scrape table from\n",
    "\n",
    "### examples\n",
    "#### Tour De France = https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners\n",
    "#### Giro d'Italia = https://en.wikipedia.org/wiki/List_of_Giro_d%27Italia_general_classification_winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'INSERT WEBSITE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "soup = bs(r.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find all the links for each competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rough = soup.find_all('span', class_='fn')\n",
    "\n",
    "links = []\n",
    "text = []\n",
    "\n",
    "for link in rough:\n",
    "    a = link.find('a')\n",
    "    links.append(a.attrs['href'])\n",
    "\n",
    "for link in rough:\n",
    "    a = link.find('a').text\n",
    "    text.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from the list of competitors pages, find the name and birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "bdays = []\n",
    "\n",
    "for link in links:\n",
    "    newurl = \"http://www.wikipedia.org\" + link\n",
    "    r = requests.get(newurl)\n",
    "    soup = bs(r.content, \"lxml\")\n",
    "    bio = soup.find('h1', class_='firstHeading').getText()\n",
    "    names.append(bio)\n",
    "    bday = soup.find('span', class_='bday').getText()\n",
    "    bdays.append(bday)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = dict(zip(names, bdays))\n",
    "with open('output.csv', 'w') as f:\n",
    "    [f.write('{0},{1}\\n'.format(key, value)) for key, value in dictionary.items()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
